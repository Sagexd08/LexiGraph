# LexiGraph Backend

High-performance FastAPI backend for AI image generation with Stable Diffusion.

## ğŸš€ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Run development server
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

## ğŸ—ï¸ Architecture

```
app/
â”œâ”€â”€ api/v1/              # API endpoints
â”‚   â”œâ”€â”€ generate.py      # Image generation
â”‚   â”œâ”€â”€ system.py        # System monitoring
â”‚   â””â”€â”€ styles.py        # Style management
â”œâ”€â”€ models/              # AI model handlers
â”‚   â”œâ”€â”€ diffusion.py     # Stable Diffusion
â”‚   â”œâ”€â”€ lora.py          # LoRA models
â”‚   â””â”€â”€ cache.py         # Smart caching
â”œâ”€â”€ utils/               # Utilities
â”œâ”€â”€ config.py            # Configuration
â””â”€â”€ main.py              # FastAPI app
```

## ğŸ“– API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v1/generate` | POST | Generate images |
| `/api/v1/styles` | GET | Style presets |
| `/api/v1/system/info` | GET | System metrics |
| `/api/v1/health` | GET | Health check |

**Documentation**: http://localhost:8000/docs

## âš™ï¸ Configuration

Key environment variables:

```env
MODEL_PATH=./models/lexigraph
DEVICE=cuda
API_KEY=your-secret-key
MAX_CONCURRENT_REQUESTS=3
ENABLE_CACHING=true
```

## ğŸš€ Deployment

```bash
# Docker
docker build -t lexigraph-backend .
docker run -p 8000:8000 --gpus all lexigraph-backend

# Production
python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

## ğŸ§ª Testing

```bash
pytest tests/ -v --cov=app
```

